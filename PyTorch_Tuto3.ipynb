{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Define the network"
      ],
      "metadata": {
        "id": "fgi4rnv7e8GN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQlfk8n-TzIz",
        "outputId": "97d15714-c903-4928-a2f0-69475c533bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Réseaux de neurones convolutifs (CNN)\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        #Une couche convolutive avec un canal d'entrée (valeur de gris), 6 canaux de sorties et un noyaux 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        #Une couche convolutive avec 6 canaux car la précédente couche possède 6 canaux de sorties\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "\n",
        "        # 1ère couche connectée de taille (400x120),\n",
        "        # le 16 représente le nombre de canaux de sortie dans la dernière couche convolutive\n",
        "        # 5*5 la taille de l'image\n",
        "        # et 120 le nombre de canaux de sortie de cette couche\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "\n",
        "        # 120 représente le nombre de canaux de sortie de la précédente couche connectée\n",
        "        # et 84 le nombre de canaux de sortie de cette couche\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "\n",
        "        # 84 représente le nombre de canaux de sortie de la précédente couche connectée\n",
        "        # et 10 le nombre de canaux de sortie de cette couche et du modèle, ici le modèle prédira 10 classes différentes\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Méthode de propagation avant, c'est à dire faire passer les données d'entrée\n",
        "        dans chaques couches du réseau de neurones pour obtenir une prédiction.\n",
        "        :param input: données d'entrée du réseau de neurones\n",
        "        :return: données de sortie du réseau de neurones\n",
        "        \"\"\"\n",
        "\n",
        "        c1 = F.relu(self.conv1(input))\n",
        "        # ReLU introduit de la non-linéarité au model en remplaçant les valeurs négatives par zéro.\n",
        "        s2 = F.max_pool2d(c1, (2, 2))\n",
        "        # Max_pool2d divise la carte de caractéristique en fenêtre 2x2 en prenant dans chauqe fenêtre la valeur maximale.\n",
        "        c3 = F.relu(self.conv2(s2))\n",
        "        s4 = F.max_pool2d(c3, 2)\n",
        "        s4 = torch.flatten(s4, 1)\n",
        "        # On transforme une matrice multidimensionnelle en vecteur unidimensionnel\n",
        "        f5 = F.relu(self.fc1(s4))\n",
        "        f6 = F.relu(self.fc2(f5))\n",
        "        output = self.fc3(f6)\n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxJyHYYST6d0",
        "outputId": "d2092c28-8050-4a96-c4e9-c7cb2cf642c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On crée une niveau avec un canal (niveaux de gris) de taille 32x32\n",
        "# de valeur aléatoire de loi normale centrée réduite\n",
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzTRnRxfONJN",
        "outputId": "19d871ba-cc83-4523-91d1-895a8d75bf30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1280,  0.0220, -0.0232,  0.0365,  0.0465,  0.1030,  0.1095, -0.0656,\n",
            "          0.0241,  0.0664]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad() #On remet à zero les gradient accumulés des itérations précédentes.\n",
        "\n",
        "#Ici on utilisent un tenseur de variable alétoire normale centrée réduite\n",
        "#Normalement on devrait utiliser en situation réel le gradient de la fonction de perte\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "metadata": {
        "id": "8fBrrMXIV0yO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "m7zfYrj2e2jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # On établit des cible aléatoire pour tester\n",
        "target = target.view(1, -1)  # On modifie la forme de target pour qu'elle soit compatible avec output\n",
        "criterion = nn.MSELoss() #On définit la fonction de perte, ici MSELoss avec la bibliothèque nn\n",
        "\n",
        "loss = criterion(output, target) # On calcul la perte en comparant output avec target\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwwfN-sNe35U",
        "outputId": "d3907939-290d-4bc5-e77b-894e8fa4cdcf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3721, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n",
        "#input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> flatten\n",
        "#      -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ck-wS6QgWPf",
        "outputId": "a584dfca-8855-439a-aa0e-769378bf2bc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MseLossBackward0 object at 0x7cc2f3ef65f0>\n",
            "<AddmmBackward0 object at 0x7cc2f3ef7220>\n",
            "<AccumulateGrad object at 0x7cc2f3ef65f0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backprop"
      ],
      "metadata": {
        "id": "EE936ToSliDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad() #On réinitialise les gradients de tout les paramètres à zéro\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad) #On affiche les gradients des biais. Nulle ici car on appelle net.zero_grad() avant\n",
        "\n",
        "loss.backward() #On calcule les gradients des paramètres\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad) #On affiche les gradients des biais."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94gloWiAkLGA",
        "outputId": "82abfe52-68f3-4c24-d57a-ba35c52dc80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\n",
            "None\n",
            "conv1.bias.grad after backward\n",
            "tensor([-0.0033,  0.0033, -0.0059,  0.0049,  0.0111,  0.0020])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update the weights"
      ],
      "metadata": {
        "id": "nJwUDs9YRah6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "weight = weight - learning_rate * gradient"
      ],
      "metadata": {
        "id": "jJsD3E5cRn-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01 #Taux d'apprentissage\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate) # On met à jour les paramètres"
      ],
      "metadata": {
        "id": "lBLpQK_HllYe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# On crée un optimiseur de descente de gradient stochastique (SGD)\n",
        "# qui met à jour les paramètre du réseau avec un taux d'appretissage à 0.01\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()  #On réinitialise les gradients de tout les paramètres à zéro\n",
        "output = net(input) # On calcul l'input dans le réseau de neurones\n",
        "loss = criterion(output, target) # On calcul la perte en comparant output avec target\n",
        "loss.backward() #Calcul du gradient de la fonction de perte par rapport aux paramètres\n",
        "optimizer.step() # On met à jour les paramètres en fonction des gradients calculés et du taux d'apprentissage"
      ],
      "metadata": {
        "id": "kowOtckKa_Jx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-jzzp4Ua_3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}